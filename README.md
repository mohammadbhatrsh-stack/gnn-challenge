ğŸ§  GNN Challenge: Graph Classification with Topological Features
Overview

Welcome to the Graph Neural Networks (GNN) Challenge!
This competition focuses on graph classification using message-passing neural networks, with an emphasis on topological (structural) feature augmentation. Participants are challenged to design models that effectively combine node features, graph structure, and topological descriptors to improve classification performance.

The challenge is small, fast, yet non-trivial, and can be fully solved using methods covered in DGL Lectures 1.1â€“4.6.

ğŸ¯ Problem Statement

Given a graph 
ğº
=
(
ğ‘‰
,
ğ¸
)
G=(V,E), predict its graph-level class label.

Each graph represents a molecular structure from the MUTAG dataset.

Basic node features are provided, but the main challenge is to leverage graph topology effectively.

ğŸ§© Problem Type

Graph Classification

Supervised Learning

Binary Classification

ğŸ“š Relevant GNN Concepts (DGL 1.1â€“4.6)

This challenge can be solved using:

Message Passing Neural Networks (MPNNs)

Graph Isomorphism Networks (GIN)

Neighborhood aggregation

Graph-level readout (global mean pooling)

Structural / Topological Node Features:

Degree

Clustering Coefficient

Betweenness Centrality

PageRank

k-core number

ğŸ“¦ Dataset

Dataset: MUTAG (TUDataset)

Graphs: 188 molecular graphs

Classes: 2 (binary classification)

Nodes per graph: ~17 (average)

Edges: Undirected

Source: Automatically downloaded from TUDataset

Small enough for quick experimentation, but rich enough to benefit from structural features.

ğŸ—‚ï¸ Data Splits

The dataset is split once using a fixed random seed to ensure fair comparison:

Split	Percentage
Train	70%
Validation	10%
Test	20%

Files provided in data/:

train.csv â†’ graph indices + labels

test.csv â†’ graph indices only (labels hidden)

âš ï¸ Test labels are hidden and used only for scoring by organizers.

ğŸ“Š Objective Metric

Macro F1-score

Why Macro F1?

Sensitive to class imbalance

Encourages balanced performance across classes

Difficult to optimize directly

This is the official ranking metric.

âš™ï¸ Constraints

To keep the competition fair and focused:

âŒ No external datasets

âŒ No pretraining

âœ… Only methods covered in DGL Lectures 1.1â€“4.6

â± Models must run within 10 minutes on CPU

âœ… Any GNN architecture is allowed (GIN, GCN, GraphSAGE, etc.)

ğŸš€ Getting Started
1ï¸âƒ£ Install Dependencies
pip install -r starter_code/requirements.txt

2ï¸âƒ£ Run the Baseline Model
cd starter_code
python baseline.py


This will:

Train a simple GIN model

Generate predictions on the test set

Save a submission file to submissions/sample_submission.csv

ğŸ“¤ Submission Format

Your submission must be a CSV file with the following format:

graph_index,target
0,1
1,0
2,1
...


graph_index: Index of the graph in the dataset

target: Predicted class label (0 or 1)

ğŸ§ª Scoring

Submissions are evaluated using:

f1_score(y_true, y_pred, average="macro")


Scores are computed using a hidden test label file.

ğŸ† Leaderboard

Submissions are ranked by Macro F1-score (higher is better)

Ties are broken by submission time

Leaderboard is maintained in: leaderboard.md

ğŸ’¡ Tips for Success

Structural features matter more than you think

Experiment with different combinations of topological features

Regularization is important for small datasets

Simpler models often generalize better

ğŸ“ Repository Structure
gnn-challenge/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv
â”‚   â””â”€â”€ test.csv
â”‚
â”œâ”€â”€ starter_code/
â”‚   â”œâ”€â”€ dataset.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ baseline.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ submissions/
â”‚   â””â”€â”€ sample_submission.csv
â”‚
â”œâ”€â”€ scoring_script.py
â”œâ”€â”€ leaderboard.md
â””â”€â”€ README.md

ğŸ Step-by-Step Commands
# 1ï¸âƒ£ Go to starter_code folder
cd starter_code

# 2ï¸âƒ£ Run the baseline to generate submission
python baseline.py

# 3ï¸âƒ£ Go back to repo root
cd ..

# 4ï¸âƒ£ Check that submission exists
dir submissions

# 5ï¸âƒ£ Score the submission
python scoring_script.py submissions\sample_submission.csv


What Each Step Does:

cd starter_code â†’ enters folder with baseline.py

python baseline.py â†’ trains the model and saves submission CSV

cd .. â†’ returns to repo root

dir submissions â†’ verifies CSV presence

python scoring_script.py ... â†’ computes and prints F1-score

ğŸ“¬ Contact

For questions or clarifications, please open a GitHub Issue.

Good luck â€” and happy graph learning! ğŸ§ ğŸ“Š

ğŸ“œ License

This project is released under the MIT License.
